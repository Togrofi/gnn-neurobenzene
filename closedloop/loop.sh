#!/bin/bash

## A closed loop shell which does
## 1) call mohex-self-play produce a set of games
## 2) refine neural net on those games by running ../simplefeature3/neuralnet/resnet.py  
## 3) freeze trained neural net via ../simplefeature3/freeze_graph/freeze_graph_main.py
## 4) use nn_load to load new nn model 
## 5) Go back to step 1

## Since step 1) is rather time and computation consuming,
## e.g., 20-block version AlphaGo Zero generated over 4 million games.
## How fast is mohex-self-play on a single GPU GTX1080 computer? 
## If 1000 simulations per move, a move consumes about 1s 
## So if a game has 60 moves, then it takes 1*10^6 minutes for 1 million game
## 1 million minutes is 10**6/60.0 ~= 700 days

# We do not start from random weights and 'near' random games (note that 
# the games generated by MCTS+random weighted NN are not entirely random!
# Because of the tree search, at least moves near the end are not random,
# --- that's why bootstrapping worked! Learning from pure random games gain nothing but random!) 
# We start from a pre-trained net on MoHex 2.0 self-play games. 
# We first generated a set of games; learning begins from those games.

if [ $# -lt 7 ] ; then
    echo "Usage: $0 mohex_exe numberOfSimulationsPerMove numberOfGamesPerIteration numberOfIterations option[selfplay|selfplaytrain] numOfParallelProcess boardsize"
    echo "Note: games will be saved at storage/, named as boardsizexboardsize_simpermove_games.txt, nn model will be saved at selfplayNN/ "
    exit 1
fi 

mohex_exe=$1 
n_sim_per_move=$2 #eg 10000, each move generated by 
n_games_per_ite=$3 #eg 1000 games per iteration
total_iteration=$4 #eg 10
runoption=$5 
Npp=$6 #e.g., 10 instances in parallel
boardsize=$7
game_file_name=${boardsize}x${boardsize}_${n_sim_per_move}_games.txt

if [[ $boardsize -gt 19 || $boardsize -lt 8 ]]; then
    echo "boardsize should between [8,19], input is too large or too small"
    exit 1
fi

if [ $runoption != "selfplay" ] && [ $runoption != "selfplaytrain" ] ; then
    echo "Usage: $0 mohex_exe numberOfSimulationsPerMove numberOfGamesPerIteration numberOfIterations option[selfplay|selfplaytrain] numOfParallelProcess boardsize"
    echo "option could only be selfplay or selfplaytrain"
    echo "selfplay: only generate selfplay games"
    echo "selfplaytrain: generate games and then train the net again"
    exit 1
fi

save_dir=./storage
mkdir -p $save_dir
nn_model_dir=./selfplayNN
mkdir -p $nn_model_dir
INF=10000000
epoch_limit_per_train=2
ditherthreshold=50
dirichlet_noise=0.03
l2_regularize=0.0

echo "$n_sim_per_move sim per move; 
$n_games_per_ite games per ite;  
$total_iteration total iterations; 
runoption: $runoption; 
$Npp selfplay instances in parallel

epoch_limit: $epoch_limit_per_train;
moveselectdither_threshold: $ditherthreshold;
dirichlet_noise: $dirichlet_noise
boardsize: $boardsize
"

export PYTHONPATH=../simplefeature3/
if [ -z $game_file_name ] ; then
    game_file_name=${boardsize}x${boardsize}_selfplay_games.txt
fi

print_info() {
    echo "save_dir: $save_dir; boardsize: $boardsize; 
    nn_model_dir: $nn_model_dir; ditherthreshold: $ditherthreshold; run option: $runoption"
}

selfplay() {
    echo -e "mohex-self-play: $1 processes in parallel"
    hname=`hostname`
    N=$1
    for((i=1; i<=$N; i++))
    do
        SEED=$(($RANDOM))
        $mohex_exe --seed $SEED <$2 >./${hname}_selfplay_log.out 2>&1 &
    done
    wait 
}

for((ite=8; ite<=$total_iteration; ite++))
do
    echo "load most recent nn model at $nn_model_dir"
    previous_pb=`ls -t $nn_model_dir/*.pb | head -n1 2>/dev/null`
    if [ -z ${previous_pb} ] ; then 
        previous_pb=""
    else
        previous_pb=$previous_pb
    fi
    echo "Iteration $ite of $total_iteration, load $previous_pb for PV-MCTS self-play data generation"

    if [ ! -z "$previous_pb" -a "$previous_pb" != " " ]; then
        cp $previous_pb ../share/nn/
        echo "$previous_pb copied to ../share/nn/ "
    fi

    nn_basename=`basename $previous_pb`

    gamefile=$game_file_name
    gamefilepath=${save_dir}/$gamefile

    echo -e "nn_load $nn_basename 
    nn_load
    param_mohex max_memory 1000000000
    param_mohex reuse_subtree 0
    param_mohex extend_unstable_search 0 
    param_mohex perform_pre_search 0 
    param_mohex max_time 999
    param_mohex use_playout_const 0.0 
    param_mohex uct_bias_constant 0.0
    param_mohex progressive_bias 5.0
    boardsize $boardsize 
    param_mohex max_games $n_sim_per_move 
    param_mohex moveselect_ditherthreshold $ditherthreshold
    param_mohex root_dirichlet_prior $dirichlet_noise
    mohex-self-play $n_games_per_ite $gamefilepath 
    quit" > config$ite.htp
    selfplay $Npp  config$ite.htp
    wait 

    if [ $runoption == "selfplay" ] ; then
        continue
    fi

    n_train_games=200000
    train_file=${save_dir}/${boardsize}x${boardsize}train$ite.txt
    echo "Select most recent $n_train_games selfplay games from $gamefilepath, dump to $train_file"
    tail -$n_train_games  $gamefilepath > $train_file
    python extractor.py --input_file=$train_file --output_file=$train_file.out --boardsize=$boardsize
    wait

    previous_checkpoint=`ls -t $nn_model_dir/${boardsize}x${boardsize}*.meta | head -n1 2>/dev/null`
    if [ -z ${previous_checkpoint} ]; then
        previous_checkpoint=""
    else
        suffix=".meta"
        previous_checkpoint=${previous_checkpoint%$suffix}; #Remove suffix
    fi

    echo "Training the neural net on $train_file.out-post, 
    save nn models to $nn_model_dir; 
    logs in /tmp/train_logs.out, starting from model $previous_checkpoint"

    n_blocks=10
    n_filters_per_layer=128
    python ../simplefeature3/neuralnet/resnet.py --verbose --n_hidden_blocks=$n_blocks --label_type='prob' --input_file=${train_file}.out-post --output_dir=$nn_model_dir/ --max_train_step=$INF --resume_train --previous_checkpoint=$previous_checkpoint --epoch_limit=$epoch_limit_per_train --boardsize=$boardsize  --l2_weight=$l2_regularize --n_filters_per_layer=$n_filters_per_layer --optimizer="momentum" > /tmp/train_logs.out 2>/tmp/train.err 

    ckpt=$nn_model_dir/${boardsize}x${boardsize}train$ite.txt.out-post.ckpt-$epoch_limit_per_train

    graph=resnet_evaluate_${n_blocks}_${n_filters_per_layer}.pbtxt
    echo "Converting neural net model to constant graph, save to $nn_model_dir"
    python ../simplefeature3/freeze_graphs/freeze_graph_main.py --input_graph=$graph --checkpoint=$ckpt >/tmp/freeze_log.txt  2>&1

    epoch_limit_per_train=$((epoch_limit_per_train))
    echo "ite $ite finished"
done
